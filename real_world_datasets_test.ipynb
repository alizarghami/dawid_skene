{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a66ba800",
   "metadata": {},
   "source": [
    "Visit https://github.com/TrentoCrowdAI/crowdsourced-datasets and follow the instructions to download the datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719c32b6",
   "metadata": {},
   "source": [
    "### TODO\n",
    "\n",
    "* Handle partially labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7d2a5190",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset_path = \"../crowdsourced-datasets/binary-classification/Blue Birds/transformed_dataset.csv\"\n",
    "\n",
    "df = pd.read_csv(dataset_path).dropna(how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "05e20b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize labels\n",
    "\n",
    "# Combine unique values\n",
    "unique_values = pd.Series(pd.unique(df[['response', 'goldLabel']].dropna().values.ravel())).sort_values()\n",
    "\n",
    "# Map each unique value to a unique integer\n",
    "value_to_int = pd.Series(range(len(unique_values)), index=unique_values)\n",
    "\n",
    "# Map column to integers\n",
    "df['normal_response'] = df['response'].map(value_to_int)\n",
    "df['normal_goldLabel'] = df['goldLabel'].map(value_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59a8a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "worker_list = df['workerID'].unique()\n",
    "task_list = df['taskID'].unique()\n",
    "label_list = df['normal_goldLabel'].unique()\n",
    "\n",
    "data = df.groupby('taskID').apply(lambda x: dict(zip(x['workerID'], [x['normal_response']]))).to_dict()\n",
    "labels_dictionary = df.drop_duplicates(subset='taskID').set_index('taskID')['normal_goldLabel'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec009ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"num Tasks:\", len(task_list))\n",
    "print (\"num Observers:\", len(worker_list))\n",
    "print (\"num Classes:\", len(label_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a83c816",
   "metadata": {},
   "source": [
    "# Run Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55a36b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dawid_skene import run as run_em\n",
    "\n",
    "_, _, _, _, class_marginals, error_rates, patient_classes = run_em(data, verbose=False)\n",
    "patient_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a4fc71",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9ff81368",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d1a036",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = list(labels_dictionary.values())\n",
    "estimated_labels = np.argmax(patient_classes, axis=1)\n",
    "\n",
    "accuracy = int(100 * accuracy_score(true_labels, estimated_labels))\n",
    "\n",
    "print(\"Accuracy: %{}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0147e7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the confusion matrices\n",
    "\n",
    "confusion_matrices = []\n",
    "for value in df['workerID'].unique():\n",
    "    worker_df = df[df['workerID'] == value]\n",
    "\n",
    "    confusion_matrices.append(confusion_matrix(worker_df['normal_goldLabel'], worker_df['normal_response'], labels=label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "08e463ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frobenius_norm(matrix1, matrix2):\n",
    "    # Convert the input matrices to numpy arrays if they are not already\n",
    "    matrix1 = np.array(matrix1)\n",
    "    matrix2 = np.array(matrix2)\n",
    "    \n",
    "    # Calculate the Frobenius norm of the difference\n",
    "    difference = matrix1 - matrix2\n",
    "    norm = np.linalg.norm(difference, 'fro')\n",
    "    \n",
    "    return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fca9828",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_error_rate = np.mean([frobenius_norm(cm_true, cm_estimate) for cm_true, cm_estimate in zip(confusion_matrices, error_rates)])\n",
    "\n",
    "print(\"Average Parameter Estimation Error: {:.2f}\".format(parameter_error_rate))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
