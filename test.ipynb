{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63dc3c80",
   "metadata": {},
   "source": [
    "# Parameter Sensetivity Test\n",
    "\n",
    "Test the algorithm accuracy using different problem parameters using fake data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ed5eec",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba69480d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "from dawid_skene import run as run_em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "712352e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fake_data_generator import RandomConfusionMatrixChannel, DataGenerator\n",
    "\n",
    "def generate_sample_data(nSamples, nWorkers, nClass, truePredictionRate=.8):\n",
    "    workers = []\n",
    "    confusion_matrices = []\n",
    "    for _ in range(nWorkers):\n",
    "        worker = RandomConfusionMatrixChannel()\n",
    "        worker.train(nClass, truePredictionRate)\n",
    "        workers.append(worker)\n",
    "        confusion_matrices.append(worker.confusionMatrix)\n",
    "\n",
    "    samples = {}\n",
    "    lables = []\n",
    "    data_generator = DataGenerator(nClass)\n",
    "    for n in range(nSamples):\n",
    "        data = data_generator.generate()\n",
    "        sample = {w:[workers[w].estimate(data)] for w in range(nWorkers)}\n",
    "        samples[n] = sample\n",
    "        lables.append(data.label)\n",
    "\n",
    "\n",
    "    return (samples, lables, confusion_matrices)\n",
    "\n",
    "\n",
    "def frobenius_norm(matrix1, matrix2):\n",
    "    # Convert the input matrices to numpy arrays if they are not already\n",
    "    matrix1 = np.array(matrix1)\n",
    "    matrix2 = np.array(matrix2)\n",
    "    \n",
    "    # Calculate the Frobenius norm of the difference\n",
    "    difference = matrix1 - matrix2\n",
    "    norm = np.linalg.norm(difference, 'fro')\n",
    "    \n",
    "    return norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616f9cf7",
   "metadata": {},
   "source": [
    "## Test 1\n",
    "Sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd2fa695",
   "metadata": {},
   "outputs": [],
   "source": [
    "nWorkers = 5                                # Number of Channels\n",
    "nSamples = [50, 100, 200, 500, 1000]        # Number of Data\n",
    "nClasses = 4                                # Number of Classes\n",
    "\n",
    "truePredictionRate = .8                     # Workers true prediction rate\n",
    "nTest = 10                                  # Number of tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12bbda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_workers = nWorkers\n",
    "n_classes = nClasses\n",
    "\n",
    "average_accurace_list = []\n",
    "average_cm_distance_list = []\n",
    "for n_samples in tqdm(nSamples):\n",
    "    accuracy_list = []\n",
    "    cm_distance_list = []\n",
    "    for _ in range(nTest):\n",
    "        samples, true_labels, confusion_matrices = generate_sample_data(n_classes, n_samples, n_workers, truePredictionRate)\n",
    "        _, _, _, _, class_marginals, error_rates, patient_classes = run_em(samples, verbose=False)\n",
    "\n",
    "        estimated_labels = np.argmax(patient_classes, axis=1)\n",
    "        accuracy = 100 * accuracy_score(true_labels, estimated_labels)\n",
    "        accuracy_list.append(accuracy)\n",
    "\n",
    "        cm_distance = np.mean([frobenius_norm(cm_true, cm_estimate) for cm_true, cm_estimate in zip(confusion_matrices, error_rates)])\n",
    "        cm_distance_list.append(cm_distance)\n",
    "\n",
    "    average_accurace_list.append(np.mean(accuracy_list))\n",
    "    average_cm_distance_list.append(np.mean(cm_distance_list))\n",
    "\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "fig.suptitle('Number Of Samples')\n",
    "\n",
    "ax1.plot(nSamples, average_accurace_list)\n",
    "ax1.set_title(\"Accuracy\")\n",
    "ax1.set_xlabel(\"Number of samples\")\n",
    "ax1.set_ylabel(\"Accuracy\")\n",
    "\n",
    "ax2.plot(nSamples, average_cm_distance_list)\n",
    "ax2.set_title(\"Confusion Matrix Distance\")\n",
    "ax2.set_xlabel(\"Number of samples\")\n",
    "ax2.set_ylabel(\"Distance\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d80674b",
   "metadata": {},
   "source": [
    "# Test 2\n",
    "\n",
    "True Prediction rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3d36b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "nWorkers = 5                                # Number of Channels\n",
    "nSamples = 1000                             # Number of Data\n",
    "nClasses = 4                                # Number of Classes\n",
    "\n",
    "truePredictionRates = [.5, .6, .7, .8, .9]  # Workers true prediction rate]\n",
    "nTest = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacbdf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_workers = nWorkers\n",
    "n_samples = nSamples\n",
    "n_classes = nClasses\n",
    "\n",
    "average_accurace_list = []\n",
    "average_cm_distance_list = []\n",
    "for truePredictionRate in tqdm(truePredictionRates):\n",
    "    accuracy_list = []\n",
    "    cm_distance_list = []\n",
    "    for _ in range(nTest):\n",
    "        samples, true_labels, confusion_matrices = generate_sample_data(n_classes, n_samples, n_workers, truePredictionRate)\n",
    "        _, _, _, _, class_marginals, error_rates, patient_classes = run_em(samples, verbose=False)\n",
    "\n",
    "        estimated_labels = np.argmax(patient_classes, axis=1)\n",
    "        accuracy = 100 * accuracy_score(true_labels, estimated_labels)\n",
    "        accuracy_list.append(accuracy)\n",
    "\n",
    "        cm_distance = np.mean([frobenius_norm(cm_true, cm_estimate) for cm_true, cm_estimate in zip(confusion_matrices, error_rates)])\n",
    "        cm_distance_list.append(cm_distance)\n",
    "\n",
    "    average_accurace_list.append(np.mean(accuracy_list))\n",
    "    average_cm_distance_list.append(np.mean(cm_distance_list))\n",
    "\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "fig.suptitle('True Prediction Rate')\n",
    "\n",
    "ax1.plot(truePredictionRates, average_accurace_list)\n",
    "ax1.set_title(\"Accuracy\")\n",
    "ax1.set_xlabel(\"True prediction rate\")\n",
    "ax1.set_ylabel(\"Accuracy\")\n",
    "\n",
    "ax2.plot(truePredictionRates, average_cm_distance_list)\n",
    "ax2.set_title(\"Confusion Matrix Distance\")\n",
    "ax2.set_xlabel(\"True prediction rate\")\n",
    "ax2.set_ylabel(\"Distance\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d475d343",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
